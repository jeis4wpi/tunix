{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial demonstrates how to fine-tune the Gemma 3 model on the GSM8K math\n",
        "reasoning dataset using Proximal Policy Optimization (PPO).\n",
        "\n",
        "Proximal Policy Optimization is a reinforcement learning (RL) algorithm that has\n",
        "become a standard for aligning Large Language Models (LLMs) with human\n",
        "preferences. PPO operates on an actor-critic architecture, and its key\n",
        "innovation is a \"clipped surrogate objective\" that constrains policy updates to\n",
        "prevent large, destabilizing changes during training. This ensures a more stable\n",
        "and reliable alignment process. Implementing PPO for LLMs can be complex,\n",
        "typically requiring four models to be active in memory during training: the\n",
        "policy model being trained, a frozen reference model, a reward model to score\n",
        "outputs, and a value model to estimate future rewards. In this example, we use a\n",
        "reward function instead of a reward model, and do not use a reference model at\n",
        "all.\n",
        "\n",
        "This notebook can be run on Colab's `v6e-1` TPU."
      ],
      "metadata": {
        "id": "64A5kTFTPFvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary libraries"
      ],
      "metadata": {
        "id": "dFcrvA3DVg2x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIxciR54VU-r"
      },
      "outputs": [],
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q ipywidgets\n",
        "\n",
        "!pip install -q datasets\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q transformers\n",
        "!pip install -q grain\n",
        "!pip install -q git+https://github.com/google/tunix@test_811362001\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git\n",
        "\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "kjsBHlTGVf7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import gc\n",
        "import os\n",
        "import re\n",
        "\n",
        "from datasets import load_dataset\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "import qwix\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.auto import tqdm\n",
        "from tunix.examples.data import translation_dataset as data_lib\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma import gemma as gemma_lib\n",
        "from tunix.models.gemma3 import params as gemma3_params_lib\n",
        "from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "from tunix.rl.ppo.ppo_learner import PPOConfig, PPOLearner\n",
        "from tunix.rl.rollout import base_rollout\n",
        "from tunix.sft import metrics_logger\n",
        "from tunix.sft.peft_main import obtain_model_config"
      ],
      "metadata": {
        "id": "uOHvh6KVVXgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Let's define the configuration we are going to use."
      ],
      "metadata": {
        "id": "BOL-mAyYVldg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Data ======\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "# ====== Reproducibility ======\n",
        "SEED = 42\n",
        "\n",
        "# ====== LoRA (for Actor) ======\n",
        "RANK = 64\n",
        "ALPHA = 64.0\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = [(1, 1), (\"fsdp\", \"tp\")]\n",
        "\n",
        "# ====== PPO ======\n",
        "# ====== Generation during PPO training ======\n",
        "MAX_PROMPT_LENGTH = 512\n",
        "TOTAL_GENERATION_STEPS = 512\n",
        "# Important to keep a high-ish temperature for varied, diverse responses during\n",
        "# training.\n",
        "TEMPERATURE = 0.9\n",
        "TOP_P = 1.0  # implies we don't do nucleus sampling\n",
        "TOP_K = 50\n",
        "\n",
        "# ====== Other PPO configs ======\n",
        "# Number of internal PPO loops.\n",
        "NUM_PPO_EPOCHS = 2\n",
        "# No KL divergence used.\n",
        "BETA = 0.0\n",
        "# Epsilon value for clipping policy loss, for stable updates.\n",
        "EPSILON = 0.2\n",
        "# Discount factor for future rewards in GAE.\n",
        "GAMMA = 1.0\n",
        "# Lambda parameter for GAE.\n",
        "GAE_LAMBDA = 1.0\n",
        "# Range for clipping the value function loss.\n",
        "CLIP_RANGE_VALUE = 0.5\n",
        "\n",
        "# ====== Training ======\n",
        "ROLLOUT_BATCH_SIZE = 16\n",
        "MINI_BATCH_SIZE = 8\n",
        "TRAINING_MICRO_BATCH_SIZE = 2\n",
        "ROLLOUT_MICRO_BATCH_SIZE = 4\n",
        "COMPUTE_LOGPS_MICRO_BATCH_SIZE = 4\n",
        "\n",
        "NUM_BATCHES = 10\n",
        "# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n",
        "# increased to a max. of 330 (if batch size is 4).\n",
        "NUM_TEST_BATCHES = 10\n",
        "EVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n",
        "# Not to be confused with `num_ppo_epochs`.\n",
        "NUM_EPOCHS = 1\n",
        "\n",
        "# Number of training steps.\n",
        "MAX_STEPS = int(\n",
        "    NUM_BATCHES\n",
        "    * TRAIN_FRACTION\n",
        "    * NUM_EPOCHS\n",
        "    * ROLLOUT_BATCH_SIZE\n",
        "    // MINI_BATCH_SIZE\n",
        ")\n",
        "\n",
        "# ====== Optimizers \u0026 Schedulers ======\n",
        "ACTOR_LEARNING_RATE = 1e-6\n",
        "CRITIC_LEARNING_RATE = 1e-5\n",
        "B1, B2 = 0.9, 0.999\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "# ====== Checkpoint Saving ======\n",
        "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
        "SAVE_INTERVAL_STEPS = 5\n",
        "MAX_TO_KEEP = 4\n",
        "DO_MEM_PROFILING = False\n",
        "\n",
        "# ====== Inference ======\n",
        "GENERATION_CONFIGS = {\n",
        "    # greedy search\n",
        "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
        "    # some randomness\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    # liberal\n",
        "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
        "}\n",
        "CACHE_SIZE = 1024"
      ],
      "metadata": {
        "id": "5c0AVJkDVXiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "c_EyjuBgWZlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_hbm_usage():\n",
        "  \"\"\"Displays memory usage per device.\"\"\"\n",
        "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "\n",
        "  for d in jax.local_devices():\n",
        "    stats = d.memory_stats()\n",
        "    used = stats[\"bytes_in_use\"]\n",
        "    limit = stats[\"bytes_limit\"]\n",
        "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
      ],
      "metadata": {
        "id": "-KiFkz_OVXkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n",
        "Fist, let's define a system prompt for the model. We instruct the model to think\n",
        "step-by-step, and then produce the final answer."
      ],
      "metadata": {
        "id": "B9WZJ7xfWc-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    'Let\\'s think step by step and output the final answer after \"####\".'\n",
        ")\n",
        "\n",
        "TEMPLATE = \"\"\"\u003cstart_of_turn\u003euser\n",
        "{question} {system_prompt}\u003cend_of_turn\u003e\n",
        "\u003cstart_of_turn\u003emodel\"\"\""
      ],
      "metadata": {
        "id": "iE4BsxdZVXm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use OpenAI's [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k).\n",
        "GSM8K comprises grade school math word problems."
      ],
      "metadata": {
        "id": "N2YQCAOyWwn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_solution(solution_str):\n",
        "  solution = re.search(\"#### (\\\\-?[0-9\\\\.\\\\,]+)\", solution_str)\n",
        "  assert solution is not None\n",
        "  final_solution = solution.group(0)\n",
        "  final_solution = final_solution.split(\"#### \")[1].replace(\",\", \"\")\n",
        "  return final_solution\n",
        "\n",
        "\n",
        "def get_dataset(data_dir, split=\"train\") -\u003e grain.MapDataset:\n",
        "  # Download data\n",
        "  if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "  data = tfds.data_source(\n",
        "      \"gsm8k\",\n",
        "      split=split,\n",
        "      data_dir=data_dir,\n",
        "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
        "      download=True,\n",
        "  )\n",
        "\n",
        "  dataset = (\n",
        "      grain.MapDataset.source(data)\n",
        "      .shuffle(seed=42)\n",
        "      .map(\n",
        "          lambda x: {\n",
        "              # passed to model forward pass\n",
        "              \"prompts\": TEMPLATE.format(\n",
        "                  system_prompt=SYSTEM_PROMPT,\n",
        "                  question=x[\"question\"].decode(\"utf-8\"),\n",
        "              ),\n",
        "              # passed to reward functions\n",
        "              \"question\": x[\"question\"].decode(\"utf-8\"),\n",
        "              # passed to reward functions\n",
        "              \"answer\": extract_solution(x[\"answer\"].decode(\"utf-8\")),\n",
        "          }\n",
        "      )\n",
        "  )\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "kdYpILGwVXpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the dataset into train and validation datasets, and also load the test dataset."
      ],
      "metadata": {
        "id": "gwJsQqw6XGUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = get_dataset(TRAIN_DATA_DIR, \"train\").batch(ROLLOUT_BATCH_SIZE)[\n",
        "    :NUM_BATCHES\n",
        "]\n",
        "\n",
        "if TRAIN_FRACTION == 1.0:\n",
        "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
        "  val_dataset = None\n",
        "else:\n",
        "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "\n",
        "  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n",
        "\n",
        "test_dataset = get_dataset(TEST_DATA_DIR, \"test\").batch(ROLLOUT_BATCH_SIZE)[\n",
        "    :NUM_TEST_BATCHES\n",
        "]\n",
        "\n",
        "len(train_dataset), len(val_dataset) if val_dataset is not None else 0, len(\n",
        "    test_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "yZt2c6GtVXri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how one batch of the dataset looks like!"
      ],
      "metadata": {
        "id": "IBXrxT4yXRDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ele in train_dataset[:1]:\n",
        "  print(ele)"
      ],
      "metadata": {
        "id": "2mTQ8PC5XTFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Policy, Critic Models\n",
        "\n",
        "In PPO, we use four models:\n",
        "\n",
        "- The policy model is the live language model being trained.\n",
        "  It's initialized from a supervised fine-tuned (SFT) model, and its weights are\n",
        "  updated to generate responses that maximize a reward.\n",
        "- The reference model is a frozen, initial copy of the SFT model. It's used to\n",
        "  calculate a KL divergence penalty that is subtracted from the reward score\n",
        "  (`reward - KL_penalty`). This acts as a regularizer, preventing the policy\n",
        "  from deviating too far from the SFT model's coherent style.\n",
        "- The critic model, or value model, is also trained alongside the policy. Its\n",
        "job is to predict the expected future reward (the \"value\") from a given\n",
        "sequence. This value is essential for calculating advantages â€” a signal that\n",
        "tells the policy if its actions were better or worse than average, helping it\n",
        "learn more efficiently.\n",
        "- The reward model to compute the reward.\n",
        "\n",
        "In this example, we skip the reference model, and use a reward function instead\n",
        "of a reward model.\n",
        "\n",
        "Note: We perform full precision (fp32) training. You can, however, leverage Qwix\n",
        "for QAT.\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and\n",
        "need to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.commodels/google/gemma/flax/). Instead of logging in,\n",
        "we recommend using Colab Secrets. This way, you don't have to manually enter\n",
        "your username and password every time you run the notebook."
      ],
      "metadata": {
        "id": "LCnSZCvnXWUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in (no need to fill this in if you've set up Colab Secrets)\n",
        "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "  kagglehub.login()"
      ],
      "metadata": {
        "id": "QMS87In3XuFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = {\n",
        "    \"gemma3\": \"google/gemma-3/flax/\",\n",
        "}\n",
        "model_family = \"gemma3\"\n",
        "model_params = \"gemma3-1b\"\n",
        "model_version = \"gemma3-1b-it\"\n",
        "\n",
        "print(f\"{model_path[model_family]}{model_version}\")\n",
        "\n",
        "kaggle_ckpt_path = kagglehub.model_download(\n",
        "    f\"{model_path[model_family]}{model_version}\"\n",
        ")"
      ],
      "metadata": {
        "id": "FSG6OVVgXyKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_model(kaggle_ckpt_path):\n",
        "  mesh = None\n",
        "  if MESH is not None:\n",
        "    mesh = jax.make_mesh(*MESH)\n",
        "\n",
        "  model_config = obtain_model_config(model_params)\n",
        "  with mesh:\n",
        "    ref_model = gemma3_params_lib.create_model_from_checkpoint(\n",
        "        os.path.join(kaggle_ckpt_path, model_version), model_config, mesh\n",
        "    )\n",
        "  return ref_model, mesh, model_config\n",
        "\n",
        "\n",
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = qwix.LoraProvider(\n",
        "      module_path=(\n",
        "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
        "          \".*attn_vec_einsum\"\n",
        "      ),\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = qwix.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  if mesh is not None:\n",
        "    with mesh:\n",
        "      state = nnx.state(lora_model)\n",
        "      pspecs = nnx.get_partition_spec(state)\n",
        "      sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "      nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ],
      "metadata": {
        "id": "Cof6p8AFXy25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy\n",
        "actor_model, mesh, model_config = get_base_model(kaggle_ckpt_path)\n",
        "actor_model = get_lora_model(actor_model, mesh=mesh)"
      ],
      "metadata": {
        "id": "7BvstAqkXy47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value\n",
        "critic_model, _, _ = get_base_model(kaggle_ckpt_path)\n",
        "critic_model = gemma_lib.TransformerWithScoreHead(\n",
        "    critic_model,\n",
        "    rngs=nnx.Rngs(params=jax.random.key(SEED)),\n",
        "    shd_cfg=('fsdp', None),\n",
        ")"
      ],
      "metadata": {
        "id": "yI18wWYLXy7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_hbm_usage()"
      ],
      "metadata": {
        "id": "kxCkyLA5YZi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define reward function\n",
        "\n",
        "The reward function is designed such that the reward = 1 if the answer is correct, 0.1 if the answer is wrong but the format is correct, and 0 in all other cases.\n",
        "\n",
        "This function has been picked from [here](https://github.com/volcengine/verl/tree/main/examples/ppo_trainer)."
      ],
      "metadata": {
        "id": "-iuGUf0GYjUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_SOLUTION_CLIP_CHARS = 300\n",
        "FORMAT_SCORE = 0.1\n",
        "CORR_ANS_SCORE = 1.0\n",
        "\n",
        "\n",
        "def extract_solution(solution_str):\n",
        "  if len(solution_str) \u003e _SOLUTION_CLIP_CHARS:\n",
        "    solution_str = solution_str[-_SOLUTION_CLIP_CHARS:]\n",
        "\n",
        "  # this also tests the formatting of the model\n",
        "  solutions = re.findall(\"#### (\\\\-?[0-9\\\\.\\\\,]+)\", solution_str)\n",
        "  if len(solutions) == 0:\n",
        "    final_answer = None\n",
        "  else:\n",
        "    # take the last solution\n",
        "    final_answer = solutions[-1].replace(\",\", \"\").replace(\"$\", \"\").strip()\n",
        "  return final_answer\n",
        "\n",
        "\n",
        "def compute_score(solution_str, ground_truth):\n",
        "  answer = extract_solution(solution_str=solution_str)\n",
        "  if answer is None:\n",
        "    return 0\n",
        "  else:\n",
        "    if answer == str(ground_truth):\n",
        "      return CORR_ANS_SCORE\n",
        "    else:\n",
        "      return FORMAT_SCORE\n",
        "\n",
        "\n",
        "def reward_fn(prompts, completions, answer, **kargs):\n",
        "  rewards = []\n",
        "  for completion, true_answer in zip(completions, answer):\n",
        "    reward = compute_score(completion, true_answer)\n",
        "    rewards.append(reward)\n",
        "  return rewards"
      ],
      "metadata": {
        "id": "vcpo2iLXYehN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test our reward function"
      ],
      "metadata": {
        "id": "IkWqV9onY0fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expected scores: 1.0, 0.1, 0.\n",
        "reward_fn(\n",
        "    [\"prompt 1\", \"prompt 2\", \"prompt 3\"],\n",
        "    [\"some thinking here #### 10\", \"no thinking #### 10\", \"wrong format\"],\n",
        "    [10, 100, 2],\n",
        ")"
      ],
      "metadata": {
        "id": "UjpieqABYyjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate\n",
        "\n",
        "Before we train the model, let's evaluate the model on the test set so we can\n",
        "see the improvement post training.\n",
        "\n",
        "We evaluate it in two ways:\n",
        "\n",
        "**Quantitative**\n",
        "\n",
        "* **Answer Accuracy**: percentage of samples for which the model predicts the\n",
        "correct final numerical answer   \n",
        "* **Format Accuracy**: percentage of samples for which the model outputs the\n",
        "correct format.\n",
        "\n",
        "**Qualitative**\n",
        "\n",
        "We'll also print outputs for a few given questions so that we can compare the\n",
        "generated output later.\n",
        "\n",
        "We define a helper function to generate an answer, given a prompt."
      ],
      "metadata": {
        "id": "rMwsqoamZHpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    question,\n",
        "    sampler,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    seed=None,\n",
        "):\n",
        "  \"\"\"Given prompt, generates text.\"\"\"\n",
        "\n",
        "  if isinstance(question, str):\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=question,\n",
        "        ),\n",
        "    ]\n",
        "  else:\n",
        "    input_batch = [\n",
        "        TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=q,\n",
        "        )\n",
        "        for q in question\n",
        "    ]\n",
        "\n",
        "  out_data = sampler(\n",
        "      input_strings=input_batch,\n",
        "      max_generation_steps=TOTAL_GENERATION_STEPS,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      echo=False,\n",
        "      seed=seed if seed is not None else None,\n",
        "  )\n",
        "\n",
        "  output = out_data.text\n",
        "  if isinstance(question, str):\n",
        "    return output[0]\n",
        "  return output"
      ],
      "metadata": {
        "id": "Rwu3DEDyYylC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    dataset,\n",
        "    sampler,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_passes=1,\n",
        "    corr_lst=False,\n",
        "    make_lst=False,\n",
        "):\n",
        "  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n",
        "\n",
        "  response_lst = []\n",
        "  corr = 0\n",
        "  corr_format = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch in tqdm(dataset):\n",
        "    answers = batch[\"answer\"]\n",
        "    questions = batch[\"question\"]\n",
        "\n",
        "    multiple_call_responses = [[] for _ in range(len(questions))]\n",
        "    for p in range(num_passes):\n",
        "      responses = generate(\n",
        "          questions, sampler, temperature, top_k, top_p, seed=p\n",
        "      )\n",
        "      for idx, response in enumerate(responses):\n",
        "        multiple_call_responses[idx].append(response)\n",
        "\n",
        "    for question, multiple_call_response, answer in zip(\n",
        "        questions, multiple_call_responses, answers\n",
        "    ):\n",
        "      # check answer\n",
        "      corr_ctr_per_question = 0\n",
        "      corr_format_per_question = 0\n",
        "      for response in multiple_call_response:\n",
        "        score = compute_score(response, answer)\n",
        "\n",
        "        if score == 1.0:\n",
        "          corr_ctr_per_question += 1\n",
        "          # If the answer is correct, the format is correct too.\n",
        "          corr_format_per_question += 1\n",
        "        # check if answer is wrong, but format is correct.\n",
        "        elif score == 0.1:\n",
        "          corr_format_per_question += 1\n",
        "\n",
        "        if corr_ctr_per_question \u003e 0 and corr_format_per_question \u003e 0:\n",
        "          break\n",
        "\n",
        "      if corr_ctr_per_question \u003e 0:\n",
        "        corr += 1\n",
        "        if corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      else:\n",
        "        if not corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      if corr_format_per_question \u003e 0:\n",
        "        corr_format += 1\n",
        "\n",
        "      total += 1\n",
        "      if total % 10 == 0:\n",
        "        print(\n",
        "            f\"===\u003e {corr=}, {total=}, {corr / total * 100=}, \"\n",
        "            f\"{corr_format / total * 100=}\"\n",
        "        )\n",
        "\n",
        "  to_return = (\n",
        "      corr,\n",
        "      total,\n",
        "      corr / total * 100,\n",
        "      corr_format / total * 100,\n",
        "  )\n",
        "  if make_lst:\n",
        "    return to_return, response_lst\n",
        "  return to_return"
      ],
      "metadata": {
        "id": "l5WqUYwsZShO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=actor_model,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=CACHE_SIZE,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "_YD9juJuZWRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(corr, total, accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(f\"{corr=}, {total=}, {accuracy=}%, {format_accuracy=}%\")"
      ],
      "metadata": {
        "id": "ZHSLbYh3ZYIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Let's set up all the configs first - checkpointing, metric logging and training. We then train the model."
      ],
      "metadata": {
        "id": "czlFhMogZauy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ckpt saving\n",
        "checkpointing_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
        ")\n",
        "\n",
        "# Metrics logger\n",
        "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/tmp/tensorboard/ppo\", flush_every_n_steps=20\n",
        ")"
      ],
      "metadata": {
        "id": "iglCqy8aZpcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tmp/tensorboard/ppo --port=0"
      ],
      "metadata": {
        "id": "X8In7CcbZYKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_optimizer(lr):\n",
        "  opt = optax.adamw(\n",
        "      learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "          init_value=0.0,\n",
        "          peak_value=lr,\n",
        "          warmup_steps=WARMUP_STEPS,\n",
        "          decay_steps=MAX_STEPS,\n",
        "          end_value=0.0,\n",
        "      ),\n",
        "      b1=B1,\n",
        "      b2=B2,\n",
        "      weight_decay=WEIGHT_DECAY,\n",
        "  )\n",
        "  if MAX_GRAD_NORM:\n",
        "    opt = optax.chain(optax.clip_by_global_norm(MAX_GRAD_NORM), opt)\n",
        "  return opt\n",
        "\n",
        "\n",
        "actor_optimizer = create_optimizer(ACTOR_LEARNING_RATE)\n",
        "critic_optimizer = create_optimizer(CRITIC_LEARNING_RATE)"
      ],
      "metadata": {
        "id": "6aYeRItUZsAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training config\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,  # Won't be used\n",
        "        rl_cluster_lib.Role.CRITIC: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=actor_optimizer,\n",
        "        critic_optimizer=critic_optimizer,\n",
        "        max_steps=MAX_STEPS,\n",
        "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "        metrics_logging_options=metrics_logging_options,\n",
        "        checkpoint_root_directory=CKPT_DIR,\n",
        "        checkpointing_options=checkpointing_options,\n",
        "        mini_batch_size=MINI_BATCH_SIZE,\n",
        "        training_micro_batch_size=TRAINING_MICRO_BATCH_SIZE,\n",
        "        rollout_micro_batch_size=ROLLOUT_MICRO_BATCH_SIZE,\n",
        "        compute_logps_micro_batch_size=COMPUTE_LOGPS_MICRO_BATCH_SIZE,\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "    ),\n",
        ")\n",
        "\n",
        "ppo_config = PPOConfig(\n",
        "    num_ppo_epochs=NUM_PPO_EPOCHS,\n",
        "    gamma=GAMMA,\n",
        "    gae_lambda=GAE_LAMBDA,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        "    clip_range_value=CLIP_RANGE_VALUE,\n",
        ")"
      ],
      "metadata": {
        "id": "5S6OdCiIZsCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RL cluster\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor=actor_model,\n",
        "    reference=actor_model,\n",
        "    critic=critic_model,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cluster_config=cluster_config,\n",
        ")\n",
        "\n",
        "# PPO trainer\n",
        "ppo_learner = PPOLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=reward_fn,\n",
        "    ppo_config=ppo_config,\n",
        "    data_shuffle_seed=SEED,\n",
        ")"
      ],
      "metadata": {
        "id": "9UEhFLS1ZyCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_learner.train(train_dataset, eval_ds=None)"
      ],
      "metadata": {
        "id": "awVTVYoVZsEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cEB3qyNUZc7S"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load checkpoint first.\n",
        "\n",
        "trained_ckpt_path = os.path.join(\n",
        "    CKPT_DIR, \"actor\", str(MAX_STEPS), \"model_params\"\n",
        ")\n",
        "\n",
        "abs_params = jax.tree.map(\n",
        "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "    nnx.state(actor_model, nnx.LoRAParam),\n",
        ")\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
        "\n",
        "nnx.update(\n",
        "    actor_model,\n",
        "    jax.tree.map(\n",
        "        lambda a, b: b,\n",
        "        nnx.state(actor_model, nnx.LoRAParam),\n",
        "        trained_lora_params,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "L6k0a-HnaO8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Ac4duThapCk"
      },
      "cell_type": "code",
      "source": [
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=actor_model,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=CACHE_SIZE,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "9uX2pr5Za2Oq"
      },
      "cell_type": "code",
      "source": [
        "(corr, total, accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(f\"{corr=}, {total=}, {accuracy=}%, {format_accuracy=}%\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "8JELymqIa5cJ"
      },
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ]
}
